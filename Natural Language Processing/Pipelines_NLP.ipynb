{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:white; background-color: black; padding: 20px; border-radius:8px; font-size:26px\"><b style=\"font-weight: 700;\"><center>LEARNING NLP </center></b></div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:white; background-color: black; padding: 20px; border-radius:8px; font-size:20px\"><b style=\"font-weight: 700;\"><center> Creating a Pipeline </center></b></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color:  #eddcd2; padding: 10px;\">\n",
    "\n",
    "### Experimental Data\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Natural Language Processing with Disaster Tweets from [here](https://www.kaggle.com/code/nkitgupta/text-representations/input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Set the encoding to UTF-8\n",
    "# -*- coding: utf-8 -*-"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:33.992337700Z",
     "start_time": "2023-11-02T12:57:33.919799900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Python, **a class is a blueprint for creating objects**. It defines a **set of attributes (data members)** and **methods (functions)** that the objects created from the class will have."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Illustrative example of the structure of a CLASS:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class MyClass:                                                        # Defines a class named 'MyClass'\n",
    "    # Class variable (shared among all instances of this class)\n",
    "    class_variable = 10                                               # This is a class variable. ItÂ´s shared among all instances of the class\n",
    "\n",
    "    # Constructor method. It initializes the object with the provided values of 'x' and 'y'. 'self' refers to the instance itself:\n",
    "    def __init__(self, x, y):\n",
    "        # Instance variables (unique to each instance)\n",
    "        self.x = x                                                    # The instance variables hold values unique to each instance of the class.\n",
    "        self.y = y\n",
    "\n",
    "    # Instances methods. They can access and modify the attributes of the instance they are called on.\n",
    "    def add(self):\n",
    "        return self.x + self.y\n",
    "\n",
    "    def multiply(self):\n",
    "        return self.x * self.y\n",
    "\n",
    "    # Class method. It can access and modify class variables.\n",
    "    @classmethod                                                # This is a decorator used to define a class method.\n",
    "    def class_method(cls):\n",
    "        return cls.class_variable\n",
    "\n",
    "    # Static method. It doesnt take 'self' or 'cls' as its first argument, making it independent of class or instance variables.\n",
    "    @staticmethod                                               # This is a decorator used to define a static method.\n",
    "    def static_method():\n",
    "        return \"This is a static method\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:33.992337700Z",
     "start_time": "2023-11-02T12:57:33.928733400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple usage example of a class. In the following example, we create two instances (`obj1` and `obj2`) of the `MyClass`. We then *access instance variables*, *call instance methods*, *access class variables*, *call class methods*, and *call the static method*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "20\n",
      "10\n",
      "10\n",
      "This is a static method\n"
     ]
    }
   ],
   "source": [
    "# Creating instances of the class\n",
    "obj1 = MyClass(5, 3)                                                      # x = 5 and y = 3\n",
    "obj2 = MyClass(10, 2)                                                     # x = 10 and y = 2\n",
    "\n",
    "# Accessing instance variables and calling instance methods\n",
    "print(obj1.add())                                                         # Output: 8\n",
    "print(obj2.multiply())                                                    # Output: 20\n",
    "\n",
    "# Accessing class variable and calling class method\n",
    "print(MyClass.class_variable)                                             # Output: 10\n",
    "print(MyClass.class_method())                                             # Output: 10\n",
    "\n",
    "# Calling static method\n",
    "print(MyClass.static_method())                                            # Output: \"This is a static method\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:33.992337700Z",
     "start_time": "2023-11-02T12:57:33.935364600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **BaseEstimator** and **TransformerMixin**\n",
    "\n",
    "By using **BaseEstimator** and **TransformerMixin**, you can create custom transformers and models that integrate seamlessly with the scikit-learn ecosystem.\n",
    "\n",
    "**BaseEstimator**:\n",
    "- BaseEstimator is a **base class for all estimators (models) in scikit-learn**. It provides **default implementations for common methods** like get_params() and set_params().\n",
    "- By inheriting from BaseEstimator, a custom estimator gains useful functionality, like the ability to set hyperparameters using keyword arguments when initializing the estimator.\n",
    "- Custom estimators can override these methods to provide custom behavior.\n",
    "\n",
    "---\n",
    "\n",
    "**TransformerMixin**\n",
    "- TransformerMixin is **another base class provided by scikit-learn. It extends BaseEstimator and adds methods specific to transformers**.\n",
    "- **Transformers are estimators that have a transform method, which takes data and returns transformed data**. For example, data preprocessing steps like standardization or one-hot encoding are typically implemented as transformers.\n",
    "- By inheriting from TransformerMixin, a custom transformer gains additional functionality, such as the ability to chain transformers together using pipelines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Usage Example of BaseEstimator and TransformerMixin (do not run the cell!!):**\n",
    "\n",
    "The class **CustomTranformer** defines **fit** and **transform** methods, which are required for any transformer in scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):     # 'CustomTransformer' is a custom estimator/transformer that inherits from both 'BaseEstimator' and 'TransformerMixin'\n",
    "\n",
    "    # Constructor method\n",
    "    def __init__(self, param1, param2):\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Custom fit logic\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Custom transform logic\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:33.992337700Z",
     "start_time": "2023-11-02T12:57:33.942809900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PIPELINE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A **Pipeline in machine learning** is a way to streamline a lot of routine processes by **putting together a sequence of data processing steps**. These steps can include **data cleaning, feature extraction, normalization, and model training, among others**.\n",
    "\n",
    "The pipeline **ensures that the data flows in a sequential manner through each step**, and **the output of one step becomes the input for the next**.\n",
    "\n",
    "- <u>Sequence of Steps</u>:\n",
    "    A pipeline is essentially a list of tuples where each tuple contains a name (string) and an instance of an estimator (transformer or model). The steps are applied sequentially.\n",
    "\n",
    "- <u>Fit and Transform</u>:\n",
    "    During the training phase, the pipeline's fit() method is called. This, in turn, triggers the fit() method for each estimator in the sequence. For transformers, this typically involves learning parameters from the data. For the final estimator (usually a model), it involves training the model on the transformed data.\n",
    "\n",
    "- <u>Predict or Score</u>:\n",
    "    Once the model is trained, you can use the pipeline's predict() or score() methods. For prediction, the data is transformed through each step and then passed to the final estimator.\n",
    "\n",
    "- <u>Simplifies Workflow</u>:\n",
    "    Pipelines simplify the workflow and help ensure that the same sequence of steps is applied to both the training data and any new data that you want to make predictions on.\n",
    "\n",
    "- <u>Avoids Data Leakage</u>:\n",
    "    Pipelines can help avoid data leakage in situations where you're using techniques like cross-validation. Each step in the pipeline operates on the data it receives and doesn't have access to the entire dataset.\n",
    "\n",
    "- <u>Code Reproducibility</u>:\n",
    "    Using pipelines ensures that all the preprocessing steps and model training are encapsulated in one object. This makes it easy to reproduce the entire workflow later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:black; background-color:  #c9e5b5; padding: 10px;\">\n",
    "\n",
    "### 1. **PIPELINE including a CLASS for CLEANING and BASIC PREPROCESSING in an NLP project**\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import string\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:33.993334Z",
     "start_time": "2023-11-02T12:57:33.949374100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. **<u>Clean text</u>**\n",
    "\n",
    "Create a **custom transformer CLASS** that inherits from **BaseEstimator** and **TransformerMixin**, indicating that it can be used as a part of a scikit-learn pipeline as an estimator, and has access to methods like **fit()** and **transform()**. When fit_transform is called on this transformer, it applies the cleaning processes to the input data and returns the modified data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Class for cleaning text:\n",
    "class text_cleaning(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):                             # The 'fit' method is a required method for any estimator in scikit-learn. In this case, its overridden to do nothing (it returns 'self'), as the cleaning process doesnt require any fitting or training.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):                               # The 'transform' method is also required for transformers in scikit-learn. It takes an input 'X' (tipycally a DataFrame or an array) and applies a series of text-cleaning functions to it.\n",
    "        # Define all the functions necessary for the text cleaning and are part of the transformer. Each of these functions takes a string of text and applies a specific cleaning operation using regular expressions or string manipulation:\n",
    "        # Remove url text\n",
    "        def remove_URL(text):\n",
    "            url = re.compile(r'http?://\\S+|www\\.\\S+')\n",
    "            return url.sub(r'',text)\n",
    "\n",
    "        # Remove html text\n",
    "        def remove_html(text):\n",
    "            html = re.compile(r'<.*?>')\n",
    "            return html.sub(r'',text)\n",
    "\n",
    "        # Remove emojis\n",
    "        def remove_emoji(text):\n",
    "            emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "            return emoji_pattern.sub(r'',text)\n",
    "\n",
    "        # Remove punctuations\n",
    "        def remove_punct(text):\n",
    "            table = str.maketrans('','',string.punctuation)\n",
    "            return text.translate(table)\n",
    "\n",
    "        # Remove newlines\n",
    "        def remove_newline(text):\n",
    "            newline = re.compile(r'\\n')\n",
    "            return newline.sub(r'',text)\n",
    "\n",
    "        # # Remove extra spaces\n",
    "        # def remove_extra_space(text):\n",
    "        #     return re.sub(r'\\S+',' ', text)\n",
    "\n",
    "        # Apply each of the cleaning functions defined above to the 'text' column of the input 'X'. This means that each cleaning operation is performed on every text sample in the dataset:\n",
    "        X['text'] = X['text'].apply(remove_URL)\n",
    "        X['text'] = X['text'].apply(remove_html)\n",
    "        X['text'] = X['text'].apply(remove_emoji)\n",
    "        X['text'] = X['text'].apply(remove_punct)\n",
    "        X['text'] = X['text'].apply(remove_newline)\n",
    "        # X['text'] = X['text'].apply(remove_extra_space)\n",
    "        X['text'] = X['text'].apply(lambda x: x.lower())       # Lower casing each word.\n",
    "\n",
    "        # Return the cleaned 'X':\n",
    "        return X\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.048869300Z",
     "start_time": "2023-11-02T12:57:33.954933300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. **<u>Tokenize and remove stop words in text</u>**\n",
    "\n",
    "Create a **custom transformer CLASS** that inherits from **BaseEstimator** and **TransformerMixin**, indicating that it can be used as a part of a scikit-learn pipeline as an estimator, and has access to methods like **fit()** and **transform()**. When fit_transform is called on this transformer, it applies the stop words removal process to the input data and returns the modified data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class stop_word_removal(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        def remove_stop_words(text):\n",
    "            processed_text = []\n",
    "            doc = nlp(text)                                 # This applies spaCy's NLP pipeline to the input text, tokenizing it and performing various linguistic analyses.\n",
    "            for token in doc:\n",
    "                if token.is_stop:                           # This checks if the token is a stop word. If the token is not a stop word, its text (the original word) is appended to processed_text.\n",
    "                    continue\n",
    "                processed_text.append(token.text)\n",
    "\n",
    "            return ' '.join(processed_text)                # The function returns a string which is the result of joining the processed tokens with spaces in between.\n",
    "\n",
    "        X['text'] = X['text'].apply(remove_stop_words)\n",
    "\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.061826Z",
     "start_time": "2023-11-02T12:57:34.012962800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. **<u>Tokenize and Lemmatize text</u>**\n",
    "\n",
    "Create a **custom transformer CLASS** that inherits from **BaseEstimator** and **TransformerMixin**, indicating that it can be used as a part of a scikit-learn pipeline as an estimator, and has access to methods like **fit()** and **transform()**. When fit_transform is called on this transformer, it applies the lemmatization process to the input data and returns the modified data:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class lemmatization(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        def lemma_func(text):\n",
    "            processed_text = []\n",
    "            doc = nlp(text)                         # This applies spaCy's natural language processing pipeline to the input text. It tokenizes the text, performs part-of-speech tagging, dependency parsing, and more.\n",
    "            for token in doc:\n",
    "                processed_text.append(token.lemma_)\n",
    "\n",
    "            return ' '.join(processed_text)         # The function returns a string which is the result of joining the processed tokens with spaces in between.\n",
    "\n",
    "        X['text'] = X['text'].apply(lemma_func)\n",
    "\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.125502600Z",
     "start_time": "2023-11-02T12:57:34.017269200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. **<u>Define Pipeline</u>**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "pipe_cleanprep = Pipeline([('clean_text', text_cleaning()),\n",
    "                           ('removing_stop_words', stop_word_removal()),\n",
    "                           ('lemma_text', lemmatization()),\n",
    "                          ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.156998700Z",
     "start_time": "2023-11-02T12:57:34.022615800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "pipe_temp = Pipeline([('clean_text', text_cleaning())\n",
    "                          ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.159066100Z",
     "start_time": "2023-11-02T12:57:34.026524200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('D:/git/Laboratory/NLP/Learning_NLP/data/train.csv')\n",
    "df_train = df_train.drop(['id', 'keyword', 'location'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:34.163042700Z",
     "start_time": "2023-11-02T12:57:34.028931800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target\n0  our deeds are the reason of this earthquake ma...       1\n1              forest fire near la ronge sask canada       1\n2  all residents asked to shelter in place are be...       1\n3  13000 people receive wildfires evacuation orde...       1\n4  just got sent this photo from ruby alaska as s...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>our deeds are the reason of this earthquake ma...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>all residents asked to shelter in place are be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>just got sent this photo from ruby alaska as s...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df_train = pipe_temp.fit_transform(df_train)\n",
    "\n",
    "cleaned_df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:57:40.169076200Z",
     "start_time": "2023-11-02T12:57:40.116747300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Apply pipe_cleanprep to training and test data**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<u> Splitted data</u>**\n",
    "\n",
    "<span style=\"color:#028553\"> **fit** and **transform** for training and test data, since the cleaning and basic preprocessing tasks are conceived so they can be applied without the dta leakage problem. </span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Load data (the data is given already splitted)\n",
    "\n",
    "df_train = pd.read_csv('D:/git/Laboratory/NLP/Learning_NLP/data/train.csv')\n",
    "df_train = df_train.drop(['id', 'keyword', 'location'],axis=1)\n",
    "\n",
    "df_test = pd.read_csv('D:/git/Laboratory/NLP/Learning_NLP/data/test.csv')\n",
    "df_test = df_test.drop(['id', 'keyword', 'location'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T10:20:43.485473700Z",
     "start_time": "2023-10-27T10:20:43.462799500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target\n0  Our Deeds are the Reason of this #earthquake M...       1\n1             Forest fire near La Ronge Sask. Canada       1\n2  All residents asked to 'shelter in place' are ...       1\n3  13,000 people receive #wildfires evacuation or...       1\n4  Just got sent this photo from Ruby #Alaska as ...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T10:21:13.725727900Z",
     "start_time": "2023-10-27T10:21:13.700433200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Apply cleaning and basic preprocessing pipeline\n",
    "\n",
    "cleaned_df_train = pipe_cleanprep.fit_transform(df_train)\n",
    "cleaned_df_test = pipe_cleanprep.fit_transform(df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T10:28:05.631972200Z",
     "start_time": "2023-10-27T10:27:21.234032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          text  target\n0                                                    1\n1                                                    1\n2                                                    1\n3                                                    1\n4                                                    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T10:30:07.519154800Z",
     "start_time": "2023-10-27T10:30:07.518150Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<u> Non-Splitted data</u>**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If data is not split\n",
    "# cleaned_df = pipe_cleanprep.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. **<u>Split data into training and test datasets</u>**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m----> 3\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mcleaned_df_train\u001B[49m\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m      4\u001B[0m                                                     cleaned_df_train\u001B[38;5;241m.\u001B[39mtarget,\n\u001B[0;32m      5\u001B[0m                                                     stratify\u001B[38;5;241m=\u001B[39mcleaned_df_train\u001B[38;5;241m.\u001B[39mtarget,            \u001B[38;5;66;03m#stratify allow the split to be split in a way that balances the train and test set by the target column\u001B[39;00m\n\u001B[0;32m      6\u001B[0m                                                     test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,                               \u001B[38;5;66;03m# 20% of data will be used as test data\u001B[39;00m\n\u001B[0;32m      7\u001B[0m                                                     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)                             \u001B[38;5;66;03m# setting random_state allow the split to be the same everytime you run this\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cleaned_df_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_df_train.text,\n",
    "                                                    cleaned_df_train.target,\n",
    "                                                    stratify=cleaned_df_train.target,            #stratify allow the split to be split in a way that balances the train and test set by the target column\n",
    "                                                    test_size=0.2,                               # 20% of data will be used as test data\n",
    "                                                    random_state=42)                             # setting random_state allow the split to be the same everytime you run this\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T09:07:06.783982Z",
     "start_time": "2023-10-27T09:07:06.635112900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:black; background-color:  #c9e5b5; padding: 10px;\">\n",
    "\n",
    "### 3. **PIPELINE including TEXT REPRESENTATION and ML MODEL in an NLP project**\n",
    "\n",
    "</div>\n",
    "\n",
    "A **Pipeline** can also be used to apply the **text representation** stage. In the following cells, I will define and apply several models for word embeddings. Each result can be tested by applying a ML model to the processed text, so I can have some kind of insight/experimentation with what kind of impact of using different models for word embeddings will have the best **accuracy**.\n",
    "\n",
    "<span style=\"color:#028553\"> The text is assumed to be already cleaned (apply pipe_cleanprep)</span>\n",
    "\n",
    "<span style=\"color:#028553\"> The following cases are examples. In principle, any word embedding method can be used</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Necessary Pkgs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T11:40:43.045998400Z",
     "start_time": "2023-10-26T11:40:42.769407400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Experimental Case: Bag of Words + Multinomial Naive Bayes**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      2\u001B[0m clf_bog_nb \u001B[38;5;241m=\u001B[39m Pipeline([(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbag-of-2-grams\u001B[39m\u001B[38;5;124m'\u001B[39m, CountVectorizer(ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m))),\n\u001B[0;32m      3\u001B[0m                        (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultiNB\u001B[39m\u001B[38;5;124m'\u001B[39m, MultinomialNB())\n\u001B[0;32m      4\u001B[0m                       ])\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# fit pipeline to training data\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m clf_bog_nb\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train\u001B[49m, y_train)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# create predictions with the fitted pipeline on the test data\u001B[39;00m\n\u001B[0;32m     10\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m clf_bog_nb\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "clf_bog_nb = Pipeline([('bag-of-2-grams', CountVectorizer(ngram_range=(1,2))),\n",
    "                       ('multiNB', MultinomialNB())\n",
    "                      ])\n",
    "\n",
    "# fit pipeline to training data\n",
    "clf_bog_nb.fit(X_train, y_train)\n",
    "\n",
    "# create predictions with the fitted pipeline on the test data\n",
    "y_pred = clf_bog_nb.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T09:04:28.452926Z",
     "start_time": "2023-10-27T09:04:28.321270300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking predictions with the Confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(clf_bog_nb,\n",
    "                                      X_test,\n",
    "                                      y_test,\n",
    "                                      values_format='d',\n",
    "                                      display_labels=['Fake news','Real news'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
